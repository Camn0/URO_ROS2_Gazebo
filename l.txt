Di bawah ini adalah struktur direktori lengkap beserta _full code_ untuk proyek tugas besar robot sederhana yang dibuat menggunakan ROS2 dan simulasi Gazebo. Proyek ini memiliki tiga paket utama:

- **robot_description**: Berisi model robot (menggunakan URDF/XACRO) beserta launch file untuk menampilkan model di Gazebo.  
- **robot_control**: Berisi node untuk pendeteksian objek (dengan OpenCV), implementasi PID, dan kontrol gerakan robot. Node PID menerima error dari pendeteksian objek, menghitung sinyal kontrol, dan _movement control_ menerjemahkannya menjadi perintah kecepatan.  
- **robot_navigation**: Berisi algoritma pathfinding A* (sebagai _placeholder_ untuk pengembangan di masa depan) beserta launch file-nya.

Pada akhir proyek, Anda juga akan mengunggah dokumentasi lengkap (misalnya file README.md) dan video simulasi ke GitHub.

Berikut adalah struktur direktori dan full code untuk masing-masing file:

---

## Struktur Direktori Proyek

```
robot_ws/
└── src/
    ├── robot_description/
    │   ├── urdf/
    │   │   └── robot.urdf.xacro
    │   ├── launch/
    │   │   └── display.launch.py
    │   ├── package.xml
    │   └── CMakeLists.txt
    ├── robot_control/
    │   ├── src/
    │   │   ├── object_detection.py
    │   │   ├── pid_controller.py
    │   │   └── movement_control.py
    │   ├── launch/
    │   │   └── control.launch.py
    │   ├── package.xml
    │   └── CMakeLists.txt
    ├── plate/
    │   └── plate.sdf
    └── robot_navigation/
        ├── src/
        │   └── a_star.py
        ├── launch/
        │   └── navigation.launch.py
        ├── package.xml
        └── CMakeLists.txt
```

---

## 1. Paket robot_description

### File: robot_ws/src/robot_description/urdf/robot.urdf.xacro

```xml
<?xml version="1.0"?>
<robot name="simple_robot" xmlns:xacro="http://www.ros.org/wiki/xacro">
  <!-- Materials -->
  <material name="chassis_color">
    <color rgba="0.8 0.2 0.2 1"/> <!-- A lively red -->
  </material>
  <material name="wheel_color">
    <color rgba="0.1 0.1 0.1 1"/> <!-- Dark, like rubber -->
  </material>
  <material name="camera_color">
    <color rgba="0.0 0.0 1.0 1"/> <!-- Blue for the camera -->
  </material>

  <!-- Chassis / Base Link -->
  <link name="base_link">
    <visual>
      <origin xyz="0 0 0.3"/>
      <geometry>
        <box size="0.6 0.5 0.2"/>
      </geometry>
      <material name="chassis_color"/>
    </visual>
    <collision>
      <origin xyz="0 0 0.3"/>
      <geometry>
        <box size="0.6 0.5 0.2"/>
      </geometry>
    </collision>
    <inertial>
      <origin xyz="0 0 0.3"/>
      <mass value="5.0"/>
      <inertia ixx="0.1" ixy="0" ixz="0" iyy="0.1" iyz="0" izz="0.1"/>
    </inertial>
  </link>
  <gazebo reference="base_link">
    <material>Gazebo/Red</material>
  </gazebo>

  <!-- Front Left Wheel -->
  <link name="wheel_front_left">
    <visual>
      <geometry>
        <cylinder radius="0.1" length="0.05"/>
      </geometry>
      <material name="wheel_color"/>
    </visual>
    <collision>
      <geometry>
        <cylinder radius="0.1" length="0.05"/>
      </geometry>
    </collision>
    <inertial>
      <mass value="0.5"/>
      <inertia ixx="0.001" ixy="0" ixz="0" iyy="0.001" iyz="0" izz="0.001"/>
    </inertial>
  </link>
  <gazebo reference="wheel_front_left">
    <material>Gazebo/Black</material>
  </gazebo>

  <!-- Front Right Wheel -->
  <link name="wheel_front_right">
    <visual>
      <geometry>
        <cylinder radius="0.1" length="0.05"/>
      </geometry>
      <material name="wheel_color"/>
    </visual>
    <collision>
      <geometry>
        <cylinder radius="0.1" length="0.05"/>
      </geometry>
    </collision>
    <inertial>
      <mass value="0.5"/>
      <inertia ixx="0.001" ixy="0" ixz="0" iyy="0.001" iyz="0" izz="0.001"/>
    </inertial>
  </link>
  <gazebo reference="wheel_front_right">
    <material>Gazebo/Black</material>
  </gazebo>

  <!-- Rear Left Wheel -->
  <link name="wheel_rear_left">
    <visual>
      <geometry>
        <cylinder radius="0.1" length="0.05"/>
      </geometry>
      <material name="wheel_color"/>
    </visual>
    <collision>
      <geometry>
        <cylinder radius="0.1" length="0.05"/>
      </geometry>
    </collision>
    <inertial>
      <mass value="0.5"/>
      <inertia ixx="0.001" ixy="0" ixz="0" iyy="0.001" iyz="0" izz="0.001"/>
    </inertial>
  </link>
  <gazebo reference="wheel_rear_left">
    <material>Gazebo/Black</material>
  </gazebo>

  <!-- Rear Right Wheel -->
  <link name="wheel_rear_right">
    <visual>
      <geometry>
        <cylinder radius="0.1" length="0.05"/>
      </geometry>
      <material name="wheel_color"/>
    </visual>
    <collision>
      <geometry>
        <cylinder radius="0.1" length="0.05"/>
      </geometry>
    </collision>
    <inertial>
      <mass value="0.5"/>
      <inertia ixx="0.001" ixy="0" ixz="0" iyy="0.001" iyz="0" izz="0.001"/>
    </inertial>
  </link>
  <gazebo reference="wheel_rear_right">
    <material>Gazebo/Black</material>
  </gazebo>

  <!-- Joints for the Four Wheels -->
  <joint name="joint_wheel_front_left" type="continuous">
    <origin xyz="0.3 0.25 0.1" rpy="-1.5708 0 0"/>
    <parent link="base_link"/>
    <child link="wheel_front_left"/>
    <axis xyz="0 1 0"/>
  </joint>

  <joint name="joint_wheel_front_right" type="continuous">
    <origin xyz="0.3 -0.25 0.1" rpy="-1.5708 0 0"/>
    <parent link="base_link"/>
    <child link="wheel_front_right"/>
    <axis xyz="0 1 0"/>
  </joint>

  <joint name="joint_wheel_rear_left" type="continuous">
    <origin xyz="-0.3 0.25 0.1" rpy="-1.5708 0 0"/>
    <parent link="base_link"/>
    <child link="wheel_rear_left"/>
    <axis xyz="0 1 0"/>
  </joint>

  <joint name="joint_wheel_rear_right" type="continuous">
    <origin xyz="-0.3 -0.25 0.1" rpy="-1.5708 0 0"/>
    <parent link="base_link"/>
    <child link="wheel_rear_right"/>
    <axis xyz="0 1 0"/>
  </joint>

  <!-- Camera Link & Sensor Plugin -->
  <link name="camera_link">
    <visual>
      <origin xyz="0 0 0"/>
      <geometry>
        <box size="0.1 0.1 0.1"/>
      </geometry>
      <material name="camera_color"/>
    </visual>
  </link>
  <gazebo reference="camera_link">
    <sensor type="camera" name="camera_sensor">
      <pose>0 0 0 0 0 0</pose>
      <always_on>true</always_on>
      <update_rate>30</update_rate>
      <camera>
        <horizontal_fov>1.047</horizontal_fov>
        <image>
          <width>640</width>
          <height>480</height>
          <format>R8G8B8</format>
        </image>
        <clip>
          <near>0.1</near>
          <far>100</far>
        </clip>
      </camera>
      <plugin name="gazebo_ros_camera" filename="libgazebo_ros_camera.so">
        <cameraName>camera</cameraName>
        <frameName>camera_link</frameName>
      </plugin>
    </sensor>
  </gazebo>

  <!-- Updated Camera Joint: Now the camera is mounted forward without an unnecessary rotation -->
  <joint name="camera_joint" type="fixed">
    <origin xyz="0.2 0 0.5" rpy="0 0 0"/>
    <parent link="base_link"/>
    <child link="camera_link"/>
  </joint>

  <!-- Diff Drive Plugin Configuration -->
  <gazebo>
    <plugin name="gazebo_ros_diff_drive" filename="libgazebo_ros_diff_drive.so">
      <ros>
        <namespace>/</namespace>
        <remapping>cmd_vel:=/cmd_vel</remapping>
      </ros>
      <left_joint>joint_wheel_front_left</left_joint>
      <left_joint>joint_wheel_rear_left</left_joint>
      <right_joint>joint_wheel_front_right</right_joint>
      <right_joint>joint_wheel_rear_right</right_joint>
      <wheel_separation>0.5</wheel_separation>
      <wheel_diameter>0.2</wheel_diameter>
      <update_rate>50</update_rate>
      <torque>5.0</torque>
    </plugin>
  </gazebo>
</robot>
```

---

### File: robot_ws/src/robot_description/launch/display.launch.py

```python
#!/usr/bin/env python3
import os
from launch import LaunchDescription
from launch_ros.actions import Node
from ament_index_python.packages import get_package_share_directory

def generate_launch_description():
    pkg_share = get_package_share_directory('robot_description')
    urdf_file = os.path.join(pkg_share, 'urdf', 'robot.urdf.xacro')
    return LaunchDescription([
        Node(
            package='gazebo_ros',
            executable='spawn_entity.py',
            arguments=['-file', urdf_file, '-entity', 'simple_robot'],
            output='screen'
        ),
    ])
```

> **Catatan:**  
> Pastikan _executable_ `spawn_entity.py` dapat mengakses file URDF yang telah dikonversi melalui xacro.

---

### File: robot_ws/src/robot_description/package.xml

```xml
<?xml version="1.0"?>
<package format="3">
  <name>robot_description</name>
  <version>0.0.1</version>
  <description>URDF description for the simple robot with camera and wheels</description>
  <maintainer email="your_email@example.com">Your Name</maintainer>
  <license>Apache-2.0</license>

  <buildtool_depend>ament_cmake</buildtool_depend>
  <depend>xacro</depend>
  <depend>gazebo_ros</depend>
  
  <export>
    <build_type>ament_cmake</build_type>
  </export>
</package>
```

---

### File: robot_ws/src/robot_description/CMakeLists.txt

```cmake
cmake_minimum_required(VERSION 3.5)
project(robot_description)

find_package(ament_cmake REQUIRED)

install(
  DIRECTORY urdf launch
  DESTINATION share/${PROJECT_NAME}
)

ament_package()
```

---

## 2. Paket robot_control

Pada paket ini terdapat tiga file utama pada direktori `src`, yaitu:

1. **object_detection.py**: Menggunakan OpenCV untuk mendeteksi objek berwarna putih dari data kamera dan menghitung error (selisih pusat gambar dengan posisi objek).  
2. **pid_controller.py**: Node yang mengimplementasikan _PID_ untuk menghitung sinyal perbaikan (angular correction) berdasarkan error yang diterima.  
3. **movement_control.py**: Mengambil sinyal perbaikan dari PID dan menerjemahkannya ke perintah _Twist_ untuk robot.

### File: robot_ws/src/robot_control/src/object_detection.py

```python
#!/usr/bin/env python3
import cv2
import numpy as np
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image
from std_msgs.msg import Float64
from cv_bridge import CvBridge

class ObjectDetectionNode(Node):
    def __init__(self):
        super().__init__('object_detection')
        # Subscribe to the camera topic (ensure the topic name matches your sensor)
        self.subscription = self.create_subscription(
            Image,
            '/camera_sensor/image_raw',
            self.image_callback,
            10
        )
        self.error_publisher = self.create_publisher(Float64, '/object_error', 10)
        self.bridge = CvBridge()

        # Declare HSV threshold parameters for green detection
        self.declare_parameter("lower_green_h", 40)
        self.declare_parameter("lower_green_s", 40)
        self.declare_parameter("lower_green_v", 40)
        self.declare_parameter("upper_green_h", 80)
        self.declare_parameter("upper_green_s", 255)
        self.declare_parameter("upper_green_v", 255)

        self.lower_green = np.array([
            self.get_parameter("lower_green_h").value,
            self.get_parameter("lower_green_s").value,
            self.get_parameter("lower_green_v").value
        ])
        self.upper_green = np.array([
            self.get_parameter("upper_green_h").value,
            self.get_parameter("upper_green_s").value,
            self.get_parameter("upper_green_v").value
        ])

    def image_callback(self, msg):
        try:
            # Convert ROS Image to OpenCV image (using "rgb8" encoding)
            cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding="rgb8")
        except Exception as e:
            self.get_logger().error(f"CV Bridge error: {e}")
            return

        # Convert image from RGB to HSV and create a mask for green regions
        hsv = cv2.cvtColor(cv_image, cv2.COLOR_RGB2HSV)
        mask = cv2.inRange(hsv, self.lower_green, self.upper_green)

        contours, _ = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
        error_msg = Float64()

        if contours:
            largest_contour = max(contours, key=cv2.contourArea)
            x, y, w, h = cv2.boundingRect(largest_contour)
            object_center = (x + w // 2, y + h // 2)
            image_center_x = cv_image.shape[1] / 2
            error = image_center_x - object_center[0]
            error_msg.data = error
            self.get_logger().info(f"Green object detected. Error: {error:.2f}")
        else:
            error_msg.data = 0.0
            self.get_logger().info("Green object not detected.")

        self.error_publisher.publish(error_msg)

def main(args=None):
    rclpy.init(args=args)
    node = ObjectDetectionNode()
    try:
        rclpy.spin(node)
    except KeyboardInterrupt:
        node.get_logger().info("Object detection node terminated")
    finally:
        node.destroy_node()
        rclpy.shutdown()

if __name__ == '__main__':
    main()
```

---

### File: robot_ws/src/robot_control/src/pid_controller.py

Pada file ini terdapat kelas PID serta node ROS yang melakukan:
- **Menerima** error (Float64) dari topik `/object_error`  
- **Menghitung** sinyal perbaikan menggunakan algoritma PID  
- **Mempublish** sinyal perbaikan (Float64) ke topik `/angular_correction`

```python
#!/usr/bin/env python3
import rclpy
from rclpy.node import Node
from std_msgs.msg import Float64

class PIDController:
    def __init__(self, kp, ki, kd):
        self.kp = kp
        self.ki = ki
        self.kd = kd
        self.previous_error = 0.0
        self.integral = 0.0

    def compute(self, error):
        self.integral += error
        derivative = error - self.previous_error
        output = self.kp * error + self.ki * self.integral + self.kd * derivative
        self.previous_error = error
        return output

class PIDControllerNode(Node):
    def __init__(self):
        super().__init__('pid_controller')
        self.subscription = self.create_subscription(
            Float64,
            '/object_error',
            self.error_callback,
            10
        )
        self.publisher_ = self.create_publisher(Float64, '/angular_correction', 10)
        
        # Declare PID parameters so they can be tuned via ROS parameters
        self.declare_parameter("kp", 0.005)
        self.declare_parameter("ki", 0.0001)
        self.declare_parameter("kd", 0.001)
        
        kp = self.get_parameter("kp").value
        ki = self.get_parameter("ki").value
        kd = self.get_parameter("kd").value
        self.pid = PIDController(kp, ki, kd)

    def error_callback(self, msg):
        error = msg.data
        correction = self.pid.compute(error)
        correction_msg = Float64()
        correction_msg.data = correction
        self.publisher_.publish(correction_msg)
        self.get_logger().info(f'Error: {error:.2f} | Angular correction: {correction:.2f}')

def main(args=None):
    rclpy.init(args=args)
    node = PIDControllerNode()
    try:
        rclpy.spin(node)
    except KeyboardInterrupt:
        node.get_logger().info('PID Controller node stopped')
    finally:
        node.destroy_node()
        rclpy.shutdown()

if __name__ == '__main__':
    main()
```

---

### File: robot_ws/src/robot_control/src/movement_control.py

Node ini berlangganan (_subscribe_) ke topik `/angular_correction` dari node PID dan menerjemahkannya menjadi pesan _Twist_ (mengatur kecepatan _linear_ dan _angular_) untuk menggerakkan robot melalui topik `/cmd_vel`.

```python
#!/usr/bin/env python3
import rclpy
from rclpy.node import Node
from geometry_msgs.msg import Twist
from std_msgs.msg import Float64

class MovementControlNode(Node):
    def __init__(self):
        super().__init__('movement_control')
        self.publisher_ = self.create_publisher(Twist, '/cmd_vel', 10)
        self.subscription = self.create_subscription(
            Float64,
            '/angular_correction',
            self.correction_callback,
            10
        )
        # Declare parameters for linear velocity and angular scaling
        self.declare_parameter("linear_velocity", 0.2)
        self.declare_parameter("angular_scale", 1.0)
        self.linear_velocity = self.get_parameter("linear_velocity").value
        self.angular_scale = self.get_parameter("angular_scale").value

        self.last_correction_time = self.get_clock().now()
        # Create a timer to check if correction messages have stopped (to perform a search maneuver)
        self.create_timer(0.5, self.timer_callback)

    def correction_callback(self, msg):
        # Update the last received time for corrections
        self.last_correction_time = self.get_clock().now()
        # Scale the angular correction if needed
        angular_correction = msg.data * self.angular_scale
        twist = Twist()
        twist.linear.x = self.linear_velocity
        twist.angular.z = angular_correction
        self.publisher_.publish(twist)
        self.get_logger().info(f'Twist: linear.x={twist.linear.x:.2f}, angular.z={twist.angular.z:.2f}')

    def timer_callback(self):
        current_time = self.get_clock().now()
        time_diff = (current_time - self.last_correction_time).nanoseconds / 1e9
        # If no corrections are received for over one second, rotate to search for the object
        if time_diff > 1.0:
            twist = Twist()
            twist.linear.x = 0.0  # Stop forward motion
            twist.angular.z = 0.2 # Rotate slowly
            self.publisher_.publish(twist)
            self.get_logger().info('No recent correction. Rotating to search for the object...')

def main(args=None):
    rclpy.init(args=args)
    node = MovementControlNode()
    try:
        rclpy.spin(node)
    except KeyboardInterrupt:
        node.get_logger().info('Movement control node terminated')
    finally:
        node.destroy_node()
        rclpy.shutdown()

if __name__ == '__main__':
    main()
```

---

### File: robot_ws/src/robot_control/launch/control.launch.py

Launch file ini menjalankan ketiga node di atas (object_detection, pid_controller, dan movement_control):

```python
#!/usr/bin/env python3
from launch import LaunchDescription
from launch_ros.actions import Node

def generate_launch_description():
    return LaunchDescription([
        Node(
            package='robot_control',
            executable='object_detection',
            name='object_detection',
            output='screen'
        ),
        Node(
            package='robot_control',
            executable='pid_controller',
            name='pid_controller',
            output='screen'
        ),
        Node(
            package='robot_control',
            executable='movement_control',
            name='movement_control',
            output='screen'
        ),
    ])

if __name__ == '__main__':
    generate_launch_description()
```

---

### File: robot_ws/src/robot_control/CMakeLists.txt

```cmake
cmake_minimum_required(VERSION 3.5)
project(robot_control)

find_package(ament_cmake REQUIRED)
find_package(rclpy REQUIRED)
find_package(sensor_msgs REQUIRED)
find_package(geometry_msgs REQUIRED)
find_package(std_msgs REQUIRED)
find_package(cv_bridge REQUIRED)
find_package(OpenCV REQUIRED)

install(PROGRAMS
  src/object_detection.py
  DESTINATION lib/${PROJECT_NAME}
  RENAME object_detection
)

install(PROGRAMS
  src/pid_controller.py
  DESTINATION lib/${PROJECT_NAME}
  RENAME pid_controller
)

install(PROGRAMS
  src/movement_control.py
  DESTINATION lib/${PROJECT_NAME}
  RENAME movement_control
)

install(
  DIRECTORY launch/
  DESTINATION share/${PROJECT_NAME}/launch
)

ament_package()
```

---

## 3. Paket robot_navigation

Pada paket ini akan diimplementasikan algoritma A* (_placeholder_) sebagai dasar navigasi.

### File: robot_ws/src/robot_navigation/src/a_star.py

```python
#!/usr/bin/env python3
def a_star(start, goal, grid):
    """
    Implementasi algoritma A* untuk mencari lintasan dari 'start' ke 'goal' pada grid.
    start: tuple (x, y)
    goal: tuple (x, y)
    grid: 2D list yang mewakili lingkungan (0: bebas, 1: rintangan)
    return: list tuple yang merupakan lintasan dari start ke goal
    """
    # IMPLEMENTASI A* dapat dikembangkan di sini
    # Sebagai placeholder, kembalikan list kosong
    return []

if __name__ == '__main__':
    # Contoh penggunaan
    start = (0, 0)
    goal = (5, 5)
    grid = [[0 for _ in range(10)] for _ in range(10)]
    path = a_star(start, goal, grid)
    print("Lintasan yang ditemukan:", path)
```

---

### File: robot_ws/src/robot_navigation/launch/navigation.launch.py

```python
#!/usr/bin/env python3
from launch import LaunchDescription
from launch_ros.actions import Node

def generate_launch_description():
    return LaunchDescription([
        # Launch node A* sebagai placeholder
        Node(
            package='robot_navigation',
            executable='a_star',
            name='a_star_navigation',
            output='screen'
        ),
    ])
```

---

### File: robot_ws/src/robot_navigation/package.xml

```xml
<?xml version="1.0"?>
<package format="3">
  <name>robot_navigation</name>
  <version>0.0.1</version>
  <description>Navigation package implementing A* pathfinding algorithm</description>
  <maintainer email="your_email@example.com">Your Name</maintainer>
  <license>Apache-2.0</license>
  
  <buildtool_depend>ament_cmake</buildtool_depend>
  <depend>rclpy</depend>
  
  <export>
    <build_type>ament_cmake</build_type>
  </export>
</package>
```

---

### File: robot_ws/src/robot_navigation/CMakeLists.txt

```cmake
cmake_minimum_required(VERSION 3.5)
project(robot_navigation)

find_package(ament_cmake REQUIRED)
find_package(rclpy REQUIRED)

install(
  DIRECTORY src/
  DESTINATION lib/${PROJECT_NAME}
)

install(
  DIRECTORY launch/
  DESTINATION share/${PROJECT_NAME}/launch
)

ament_package()
```

---

## 4. File Root-level CMakeLists.txt

Meskipun pada workspace ROS2 Anda _colcon_ akan membangun paket-paket secara terpisah, Anda dapat menyertakan file _CMakeLists.txt_ di root sebagai _placeholder_.

### File: robot_ws/CMakeLists.txt

```cmake
cmake_minimum_required(VERSION 3.5)
# File ini digunakan sebagai referensi root pada workspace.
# Paket-paket ROS2 di dalam folder src/ akan di-build secara independen dengan colcon.
```

---

## Cara Menjalankan Proyek

1. **Persiapan Lingkungan:**  
   Pastikan ROS2 dan Gazebo telah terinstal serta dependensi tambahan seperti `cv_bridge` dan `opencv`.

2. **Membangun Workspace:**  
   Buka terminal dan jalankan perintah berikut:
   ```bash
   mkdir -p ~/robot_ws/src
   # Tempatkan folder src/ dengan paket-paket di atas ke dalam ~/robot_ws/src
   cd ~/robot_ws
   colcon build
   source install/setup.bash
   ```

3. **Menjalankan Simulasi:**

   - **Spawn Robot di Gazebo:**  
     ```bash
     ros2 launch robot_description display.launch.py
     ```
     
   - **Menjalankan Sistem Kontrol:**  
     ```bash
     ros2 launch robot_control control.launch.py
     ```
     
   - (Opsional) **Menjalankan Navigasi (A*):**  
     ```bash
     ros2 launch robot_navigation navigation.launch.py
     ```

4. **Uji Sistem:**  
   Di dalam simulasi Gazebo, letakkan objek berwarna putih (misalnya sebuah kotak dengan warna putih) di depan robot. Robot akan mendeteksi objek menggunakan kamera, menghitung error, memperbaiki arah dengan PID, dan bergerak mendekati objek secara otomatis.

5. **Dokumentasi & Deliverables:**  
   Pastikan Anda melengkapi dokumentasi (misalnya README.md) yang menjelaskan:
   - Cara kerja sistem ROS,
   - Alur data dari pendeteksian objek hingga perintah gerak,
   - Video simulasi robot, dan
   - Unggah seluruh repository ke GitHub.

---

Dokumentasi dan penjelasan lengkap mengenai cara kerja sistem ROS, alur kerja masing-masing node, dan tuning parameter PID dapat Anda sertakan dalam file README.md di root repository.

Semoga struktur dan kode di atas membantu dalam pengerjaan tugas besar robot Anda! Jika ada pertanyaan atau bagian yang perlu diperjelas, jangan ragu untuk meminta bantuan lebih lanjut.

Below is a very detailed step-by-step guide to help you diagnose and resolve the issue so that your robot follows the plate in Gazebo. The overall idea is to ensure that each part of your perception–control pipeline is functioning correctly. We’ll break it down into several sections:

---

## 1. Verify Your Simulation Environment

- **Confirm the Plate Is Spawned Correctly:**  
  Make sure that the plate has been successfully spawned in your Gazebo world. You should see it in the simulation scene.  
  - If you used an SDF file to spawn the plate, double-check the file path and that you received a confirmation message in your terminal after running the spawn command.

- **Check the Plate’s Position:**  
  Ensure that the plate is within the field-of-view of your robot’s camera. If the plate is off to one side or too far away, the camera may not pick it up.  
  - You might want to temporarily reposition the plate closer to the robot to verify detection.

---

## 2. Verify the Camera Stream and Topics

Your object detection node subscribes to `/camera/image_raw`. It’s critical that your camera is correctly publishing images.

- **List Topics:**  
  Run the following command to see if the camera topic exists:  
  ```
  ros2 topic list
  ```  
  Look for `/camera/image_raw` in the output.

- **Echo Camera Images:**  
  It can be helpful to check that images are coming through:  
  ```
  ros2 topic echo /camera/image_raw
  ```  
  (Note that this might print raw binary data; if you have an image viewer like `rqt_image_view` installed you can run it with:  
  ```
  ros2 run rqt_image_view rqt_image_view
  ```
  and select `/camera/image_raw`.)

- **Review Camera Configuration in URDF:**  
  Double-check that the camera topic name (or namespace) in your URDF sensor plugin matches the topic your node subscribes to. In our sample URDF, the camera sensor was configured with a namespace like `/camera` and the frame `camera_link`.

---

## 3. Check the Object Detection Node (object_detection.py)

Your object detection node uses OpenCV to detect a white object (i.e. the plate). To make sure this is working:

- **Logging:**  
  When running the node, check the console output. Look for messages like:  
  - “Objek putih terdeteksi. Error: …”  
  - Or “Objek putih tidak terdeteksi.”  
  This log tells you whether the white object is being found.  
  - If you consistently see “tidak terdeteksi,” then the HSV thresholds may be off.

- **Adjust HSV Thresholds if Necessary:**  
  In your detection code, you have set:  

  ```python
  lower_white = np.array([0, 0, 200])
  upper_white = np.array([180, 30, 255])
  ```  
  If your plate’s color is not bright white or if lighting in Gazebo is different, adjust these values. You might want to visualize the mask by adding—temporarily—a display of the masked image using:
  
  ```python
  cv2.imshow("Mask", mask)
  cv2.waitKey(1)
  ```  
  
- **Test the Output:**  
  Publish sample images (or use screenshots from Gazebo) and run your detection code locally if you’re unsure the color thresholds fit the plate.

---

## 4. Validate Topic Communication Between Nodes

Your control system uses several topics to pass information between nodes. Verify that messages are being exchanged:

- **Check `/object_error`:**  
  Open a new terminal and run:  
  ```
  ros2 topic echo /object_error
  ```  
  - You should see numerical values. If the plate is within view, expect nonzero errors (i.e., the difference between the image center and the plate’s center).  
  - If you see only zeros or no output, the object detection node might not be detecting the plate.

- **Check `/angular_correction`:**  
  Similarly, test:  
  ```
  ros2 topic echo /angular_correction
  ```  
  - The PID controller node should be publishing a correction signal here based on the error values.

- **Check `/cmd_vel`:**  
  Finally, verify that the movement control node is publishing Twist messages:  
  ```
  ros2 topic echo /cmd_vel
  ```  
  - This topic is what Gazebo will use to drive your robot. The message should include both a `linear.x` (forward speed) and an `angular.z` (rotation rate).  

---

## 5. Check and Tune the PID Controller

If your robot isn’t turning towards the plate, the computed angular correction might be too small (or too high):

- **Review PID Gains:**  
  In your `pid_controller.py`, the gains are set as follows:
  ```python
  self.pid = PIDController(kp=0.005, ki=0.0001, kd=0.001)
  ```  
  - If the corrections are insufficient (the robot doesn’t turn), try increasing the proportional gain (`kp`).  
  - If the robot overcorrects or oscillates, you might need to reduce `kp` or adjust `ki` and `kd` for smoothing.

- **Log Computed Values:**  
  Add extra print or logger statements to output the error and the corresponding correction value. This can help you understand if the PID is working as expected.

---

## 6. Verify Movement Control Node

Ensure that the `movement_control.py` node processes the angular correction appropriately:

- **Constant Linear Velocity:**  
  The node sets a constant `twist.linear.x = 0.2`. Confirm that this speed meets your simulation’s requirements. Sometimes a low speed might make turning less apparent.

- **Mapping Angular Correction:**  
  Check that the correction from the PID node is directly mapped to `twist.angular.z`. If the robot’s steering isn’t responsive, consider scaling the angular value.

- **Confirm Wheels Are Moving:**  
  In Gazebo, observe the robot’s wheels. They should be spinning if `/cmd_vel` commands are applied correctly. If not, check that your robot’s URDF and plugins (if any) correctly map these commands to wheel actuators.

---

## 7. Verify the Overall Pipeline Step-by-Step

1. **Start Gazebo with the Robot and Plate:**
   - Launch the robot model in Gazebo:
     ```
     ros2 launch robot_description display.launch.py
     ```
   - Spawn the plate (if not already included in your world):
     ```
     ros2 run gazebo_ros spawn_entity.py -entity plate -file ~/robot_ws/src/plate/plate.sdf
     ```
     plate.sdf
     ```
     <?xml version="1.0" ?>
<sdf version="1.4">
  <model name="plate">
    <static>true</static>
    <link name="link">
      <visual name="visual">
        <geometry>
          <cylinder>
            <radius>0.15</radius>
            <length>0.02</length>
          </cylinder>
        </geometry>
        <material>
          <ambient>0 1 0 1</ambient>
          <diffuse>0 1 0 1</diffuse>
        </material>
      </visual>
      <collision name="collision">
        <geometry>
          <cylinder>
            <radius>0.15</radius>
            <length>0.02</length>
          </cylinder>
        </geometry>
      </collision>
    </link>
  </model>
</sdf>
     ```

2. **Launch Control Nodes:**
   - Run your control launch file:
     ```
     ros2 launch robot_control control.launch.py
     ```
   - You should see the log statements (as you posted) indicating that the nodes for object detection, PID controller, and movement control are running.

3. **Observe and Debug:**
   - **Check Console Logs:**  
     See if the object detection node logs detection messages. If it logs “Objek putih tidak terdeteksi” even when the plate is visible, then it’s a sign the detection thresholds or camera view are off.
     
   - **Topic Echoing:**  
     Use the `ros2 topic echo` commands mentioned above to see if the error, angular correction, and cmd_vel messages are published.
     
   - **Reposition the Plate:**  
     Try manually moving the plate into different parts of the camera’s view to see if the published error changes accordingly.

   - **Adjust Parameters on the Fly:**  
     If possible, add ROS parameters (or simply modify and rebuild) so that you can more quickly tweak HSV thresholds or PID gains.

---

## 8. Additional Tips

- **Lighting Conditions in Gazebo:**  
  Sometimes the lighting in the Gazebo simulation can affect color detection. You might need to adjust your thresholds based on the actual lighting.

- **Visual Debugging:**  
  Consider showing the processed image (with bounding boxes) using OpenCV’s GUI functions during testing so you can visually confirm what the node “sees.”

- **Plugin Use for Diff Drive:**  
  If you find that twist messages are not moving the robot (despite seeing them published), check if there’s a need to include a gazebo plugin (like `gazebo_ros_diff_drive`) in your robot description to convert twist commands into motor actions.

- **Re-Test After Each Change:**  
  After adjusting parameters or code, always re-launch your simulation and nodes to confirm changes take effect.

---

By following these detailed steps, you can systematically identify which part of the system is not working as expected—whether it’s the camera stream, object detection, PID controller, or movement control—and make the necessary adjustments so your robot can successfully follow the plate in Gazebo.

If you run into specific issues or need further assistance on a particular step (like tuning the PID gains or adjusting the HSV thresholds), feel free to ask!
